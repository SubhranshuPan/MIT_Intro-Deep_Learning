{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class MyDenseLayer (tf.keras.layers.Layer):\n",
        "  def __init__ (self, input_dim, output_dim):\n",
        "    super(MyDenseLayer, self).__init__()\n",
        "\n",
        "    # Initialise weights and biases\n",
        "    self.W = self.add_weight([input_dim, output_dim])\n",
        "    self.b = self.add_weight([1, output_dim])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Forward propagation\n",
        "    z = tf.matmul(inputs, self.W) + self.b\n",
        "\n",
        "    # Feed through a non-linear activation\n",
        "    output = tf.math.sigmoid(z)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "T7CbbQ79QpnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor Flow Shortcut\n",
        "import tensorflow as tf\n",
        "\n",
        "layer = tf.keras.layers.Dense(units=2) # dense layer with 2 outputs"
      ],
      "metadata": {
        "id": "l9CJOwZDSKcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi output perceptron\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(n), # layer 1 with n neurons\n",
        "    tf.keras.layers.Dense(2)  # layer 2 with 2 neurons\n",
        "])"
      ],
      "metadata": {
        "id": "UldofGNOSysg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Losses\n",
        "\n",
        "y = \"Output\"\n",
        "predicted = \"Predicted Value By model\"\n",
        "\n",
        "# Binary Cross Entropy Loss\n",
        "\"\"\"\n",
        "used with models that output a probability between 0 and 1\n",
        "will output 1 or 0 depending on if value is greater/less than 0.5\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(y, predicted) )\n",
        "\n",
        "# Mean squared Error Loss\n",
        "# used with regression models that output continuous real numbers\n",
        "loss = tf.reduce_mean( tf.square(tf.subtract(y, predicted)) )\n",
        "\n",
        "# Or Shortcut\n",
        "loss = tf.keras.losses.MSE(y, predicted)"
      ],
      "metadata": {
        "id": "CZJQ5pxKU1IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent\n",
        "import tensorflow as tf\n",
        "\n",
        "lr = 0.01 # Learning Rate\n",
        "\n",
        "weights = tf.Variable([tf.random.normal()])\n",
        "\n",
        "while True:\n",
        "    with tf.GradientTape() as g:\n",
        "        loss = compute_loss(weights)  # MSE or Softmax cross entropy\n",
        "        gradient = g.gradient(loss, weights)\n",
        "\n",
        "    weights = weights - gradient * lr\n",
        "\n"
      ],
      "metadata": {
        "id": "ORsYgrGeYGBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Various Gradient Descent Algorithms:\n",
        "\n",
        "tf.keras.optimizers.SGD   #stochastic\n",
        "tf.keras.optimizers.Adam\n",
        "tf.keras.optimizers.Adadelta\n",
        "tf.keras.optimizers.Adagrad\n",
        "tf.keras.optimizers.RMSProp"
      ],
      "metadata": {
        "id": "NvjbNQLwbp55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([...])\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD()\n",
        "\n",
        "while True:\n",
        "\n",
        "    prediction = model(x)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(y, prediction)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
      ],
      "metadata": {
        "id": "F98EHND6aS2t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}